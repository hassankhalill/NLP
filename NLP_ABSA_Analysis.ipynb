{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NLP Analysis of Google Reviews for Saudi Arabian Sites\n",
    "## Aspect-Based Sentiment Analysis (ABSA)\n",
    "\n",
    "This notebook implements a comprehensive NLP pipeline for analyzing Google reviews of Saudi Arabian tourism sites.\n",
    "\n",
    "**Objectives:**\n",
    "1. Data preprocessing and transformation\n",
    "2. Text cleaning and NLP analysis\n",
    "3. Sentiment analysis\n",
    "4. Exploratory Data Analysis\n",
    "5. ABSA model development\n",
    "6. Model deployment and monitoring"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Phase 1: Data Loading and Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import ast\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from collections import Counter\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set display options\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_colwidth', 100)\n",
    "\n",
    "# Set visualization style\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette('husl')\n",
    "\n",
    "print(\"Libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "df = pd.read_csv('DataSet.csv')\n",
    "\n",
    "print(f\"Dataset shape: {df.shape}\")\n",
    "print(f\"\\nColumn names: {df.columns.tolist()}\")\n",
    "print(f\"\\nFirst few rows:\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic data info\n",
    "print(\"Dataset Information:\")\n",
    "print(\"=\" * 50)\n",
    "df.info()\n",
    "\n",
    "print(\"\\n\" + \"=\" * 50)\n",
    "print(\"Missing Values:\")\n",
    "print(df.isnull().sum())\n",
    "\n",
    "print(\"\\n\" + \"=\" * 50)\n",
    "print(\"Statistical Summary:\")\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Examine sample values from key columns\n",
    "print(\"Sample tags column:\")\n",
    "print(df['tags'].iloc[0])\n",
    "print(\"\\nSample ratings column:\")\n",
    "print(df['ratings'].iloc[0])\n",
    "print(\"\\nSample content:\")\n",
    "print(df['content'].iloc[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the mappings file\n",
    "with open('Mappings.json', 'r', encoding='utf-8') as f:\n",
    "    mappings = json.load(f)\n",
    "\n",
    "tags_mapping = mappings['tags_mapping']\n",
    "\n",
    "print(f\"Total mappings available: {len(tags_mapping)}\")\n",
    "print(\"\\nSample mappings:\")\n",
    "for i, (key, value) in enumerate(list(tags_mapping.items())[:5]):\n",
    "    print(f\"{key}: {value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Phase 2: Data Preprocessing and Transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to safely parse JSON-like strings\n",
    "def safe_parse_json(json_string):\n",
    "    \"\"\"\n",
    "    Safely parse JSON or JSON-like strings.\n",
    "    Returns parsed object or None if parsing fails.\n",
    "    \"\"\"\n",
    "    if pd.isna(json_string):\n",
    "        return None\n",
    "    \n",
    "    try:\n",
    "        # Try parsing as JSON first\n",
    "        return json.loads(json_string)\n",
    "    except (json.JSONDecodeError, TypeError):\n",
    "        try:\n",
    "            # Try using ast.literal_eval for Python dict-like strings\n",
    "            return ast.literal_eval(json_string)\n",
    "        except (ValueError, SyntaxError):\n",
    "            return None\n",
    "\n",
    "print(\"Helper function defined successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parse the ratings column\n",
    "print(\"Parsing ratings column...\")\n",
    "\n",
    "df['ratings_parsed'] = df['ratings'].apply(safe_parse_json)\n",
    "df['normalized_rating'] = df['ratings_parsed'].apply(lambda x: x.get('normalized') if x else None)\n",
    "df['raw_rating'] = df['ratings_parsed'].apply(lambda x: x.get('raw') if x else None)\n",
    "\n",
    "print(f\"Ratings parsed successfully!\")\n",
    "print(f\"\\nSample parsed ratings:\")\n",
    "print(df[['ratings', 'normalized_rating', 'raw_rating']].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parse the tags column and extract hash keys\n",
    "print(\"Parsing tags column...\")\n",
    "\n",
    "df['tags_parsed'] = df['tags'].apply(safe_parse_json)\n",
    "\n",
    "# Extract hash values from tags\n",
    "def extract_hash_values(tags_list):\n",
    "    \"\"\"Extract hash values from parsed tags list.\"\"\"\n",
    "    if not tags_list or not isinstance(tags_list, list):\n",
    "        return []\n",
    "    return [tag.get('value') for tag in tags_list if isinstance(tag, dict) and 'value' in tag]\n",
    "\n",
    "df['hash_values'] = df['tags_parsed'].apply(extract_hash_values)\n",
    "\n",
    "print(\"Tags parsed successfully!\")\n",
    "print(f\"\\nSample hash values:\")\n",
    "print(df[['tags', 'hash_values']].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Map hash values to offerings and destinations\n",
    "print(\"Mapping hash values to offerings and destinations...\")\n",
    "\n",
    "def map_hash_to_attributes(hash_list, mappings_dict):\n",
    "    \"\"\"\n",
    "    Map list of hash values to offerings and destinations.\n",
    "    Returns tuple of (offerings_list, destinations_list)\n",
    "    \"\"\"\n",
    "    if not hash_list:\n",
    "        return [], []\n",
    "    \n",
    "    offerings = []\n",
    "    destinations = []\n",
    "    \n",
    "    for hash_val in hash_list:\n",
    "        if hash_val in mappings_dict:\n",
    "            mapping = mappings_dict[hash_val]\n",
    "            if len(mapping) >= 2:\n",
    "                offerings.append(mapping[0])\n",
    "                destinations.append(mapping[1])\n",
    "    \n",
    "    # Remove duplicates while preserving order\n",
    "    offerings = list(dict.fromkeys(offerings))\n",
    "    destinations = list(dict.fromkeys(destinations))\n",
    "    \n",
    "    return offerings, destinations\n",
    "\n",
    "# Apply mapping\n",
    "df[['offerings_list', 'destinations_list']] = df['hash_values'].apply(\n",
    "    lambda x: pd.Series(map_hash_to_attributes(x, tags_mapping))\n",
    ")\n",
    "\n",
    "# Create string versions for easier viewing\n",
    "df['offerings'] = df['offerings_list'].apply(lambda x: ', '.join(x) if x else '')\n",
    "df['destinations'] = df['destinations_list'].apply(lambda x: ', '.join(x) if x else '')\n",
    "\n",
    "print(\"Mapping completed successfully!\")\n",
    "print(f\"\\nSample mapped data:\")\n",
    "print(df[['title', 'offerings', 'destinations']].head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a clean working dataframe with relevant columns\n",
    "df_clean = df[[\n",
    "    'id', 'content', 'date', 'language', 'title',\n",
    "    'normalized_rating', 'raw_rating', \n",
    "    'offerings', 'destinations',\n",
    "    'offerings_list', 'destinations_list'\n",
    "]].copy()\n",
    "\n",
    "print(f\"Clean dataframe shape: {df_clean.shape}\")\n",
    "print(f\"\\nColumns: {df_clean.columns.tolist()}\")\n",
    "print(f\"\\nSample data:\")\n",
    "df_clean.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data quality checks\n",
    "print(\"Data Quality Report:\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"Total records: {len(df_clean)}\")\n",
    "print(f\"\\nMissing values:\")\n",
    "print(df_clean.isnull().sum())\n",
    "print(f\"\\nEmpty content: {(df_clean['content'].str.strip() == '').sum()}\")\n",
    "print(f\"Empty offerings: {(df_clean['offerings'] == '').sum()}\")\n",
    "print(f\"Empty destinations: {(df_clean['destinations'] == '').sum()}\")\n",
    "print(f\"\\nRating distribution:\")\n",
    "print(df_clean['raw_rating'].value_counts().sort_index())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Distribution of offerings\n",
    "from collections import Counter\n",
    "\n",
    "all_offerings = []\n",
    "for offerings_list in df_clean['offerings_list']:\n",
    "    all_offerings.extend(offerings_list)\n",
    "\n",
    "offerings_count = Counter(all_offerings)\n",
    "\n",
    "print(\"Offerings Distribution:\")\n",
    "print(\"=\" * 50)\n",
    "for offering, count in offerings_count.most_common():\n",
    "    print(f\"{offering}: {count} ({count/len(df_clean)*100:.2f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Distribution of destinations\n",
    "all_destinations = []\n",
    "for dest_list in df_clean['destinations_list']:\n",
    "    all_destinations.extend(dest_list)\n",
    "\n",
    "destinations_count = Counter(all_destinations)\n",
    "\n",
    "print(\"Destinations Distribution:\")\n",
    "print(\"=\" * 50)\n",
    "for destination, count in destinations_count.most_common():\n",
    "    print(f\"{destination}: {count} ({count/len(df_clean)*100:.2f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the preprocessed data\n",
    "df_clean.to_csv('preprocessed_data.csv', index=False)\n",
    "print(\"Preprocessed data saved to 'preprocessed_data.csv'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary of Phase 1\n",
    "\n",
    "**Completed Tasks:**\n",
    "1. ✅ Loaded dataset with 10,000 reviews\n",
    "2. ✅ Parsed JSON-encoded tags and ratings columns\n",
    "3. ✅ Mapped hash keys to offerings and destinations using mapping file\n",
    "4. ✅ Created structured columns for analysis\n",
    "5. ✅ Analyzed data quality and distributions\n",
    "\n",
    "**Key Findings:**\n",
    "- Dataset contains reviews in both Arabic and English\n",
    "- Reviews span multiple offerings (Tourism, Accommodation, F&B, etc.)\n",
    "- Multiple destinations across Saudi Arabia\n",
    "- Ratings range from 1-5 stars\n",
    "\n",
    "**Next Steps:**\n",
    "- Phase 2: Text cleaning and NLP preprocessing\n",
    "- Phase 3: Sentiment analysis implementation"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
